---
title: "MA679-Homework 2"
author: "Jinfei Xue"
date: "1/28/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 3.1

Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

*The null hypotheses associated with table 3.4 are that advertising budgets of TV, radio or newspaper do not have an effect on sales. That is to say, the null hypothesis is that any of the coefficients of TV, radio or newspaper is equal to 0.*

*From the table 3.4, we can see that the corresponding p-values are highly significant for TV and radio but not significant for newspaper. Therefore, we can conclude that the newspaper advertising budget is not associated with sales.*

## 3.2

Carefully explain the differences between the KNN classifier and KNN regression methods.

*The KNN classifier is typically used to solve classification problems (those with a qualitative response) by identifying the neighborhood of $x_0$ and then estimating the conditional probability $P(Y=j|X=x_0)$ for class j as the fraction of points in the neighborhood whose response values equal j. The KNN regression method is used to solve regression problems (those with a quantitative response) by again identifying the neighborhood of $x_0$ and then estimating $f(x_0)$ as the average of all the training responses in the neighborhood.*


## 3.5

$$\hat{y_i} = \frac{(x_1y_1+x_2y_2+...+x_ny_n)x_i}{x_1^2+x_2^2+...+x_n^2} = \frac{x_1x_i}{\sum_{j=1}^{n}x_j^2}y_1+\frac{x_2x_i}{\sum_{j=1}^{n}x_j^2}y_2+...+\frac{x_nx_i}{\sum_{j=1}^{n}x_j^2}y_1$$
$$=\sum_{i'=1}^n\frac{x_{i'}x_i}{\sum_{j=1}^nx_j^2}y_{i'}$$
$$a_{i'}=\frac{x_{i'}x_i}{\sum_{j=1}^nx_j^2}$$

## 3.6

*The least squares line equation is $y=\hat{\beta_0}+\hat{\beta_1}x$, so if we substitute $\bar{x}$ for x we obtain $y=\hat{\beta_0}+\hat{\beta_1}x=\bar{y}-\hat{\beta_1}\bar{x}+\hat{\beta_1}\bar{x}=\bar{y}$.*

*Therefore, we can conclude that the least square line passes through the point $(\bar{x},\bar{y})$.*


## 3.11

```{r}
#generate a predictor x and a response y
set.seed (1)
x=rnorm (100)
y=2*x+rnorm (100)
```

(a) regression without intercept (y onto x)

```{r}
r11_a=lm(y~x+0)
summary(r11_a)
```

*From the summary above, we can see that the coefficient of x is 1.9939, which means if x increases by 1 unit, then the value of y will approximately increase by 1.9939 units.*

*the t-value for null hypothesis is 18.73 and the corresponding p-value is much smaller than 0.05. Therefore, we can reject the null hypothesis and indicate that x is associated with y.*

*$R^2$=0.7798 is large enough to show that a large proportion of the variability in the response has been explained by the regression.*

(b) regression without intercept (x onto y)

```{r}
r11_b <- lm(x ~ y + 0)
summary(r11_b)
```

*From the summary above, we can see that the coefficient of y is 0.39111, which means if y increases by 1 unit, then the value of x will approximately increase by 0.39111 units.*

*the t-value for null hypothesis is 18.73 (which is equal to the value in (a)) and the corresponding p-value is much smaller than 0.05. Therefore, we can reject the null hypothesis and indicate that y is associated with x.*

*$R^2$=0.7798 (which is equal to the value in (a)) is large enough to show that a large proportion of the variability in the response has been explained by the regression.*

(c) What is the relationship between the results obtained in (a) and (b)?

*We obtain the same value for the t statistics and consequently the same value for the corresponding p-value, and R-squared. Both results in (a) and (b) reflect the same line created in (a).*

(d) Show the t-statistic formula algebraically and confirm it numerically in R;

Given ${t = \frac{\beta}{SE\left(\beta\right)}}$ and  ${SE\left(\beta\right)}=\sqrt{\frac{\sum{\left(y_i-\beta x_i\right)^2}}{\left(n-1\right)\sum{x_i^2}}}$ and $\beta=\frac{\sum{x_i y_i}}{\sum{x_i^2}}$

Substituting ${\beta}$ into $t$ we get: ${t=\frac{\sqrt{n-1} \sum{x_i y_i}}{\sqrt{\sum{x_i^2}}\sqrt{\sum{y_i^2-2 \sum{ y_i x_i \frac{\sum{x_i y_i}}{\sum{x_i^2}}}+\sum{\left(\frac{\sum{x_i y_i}}{\sum{x_i^2}}\right)^2 x_i^2}}}}}$

```{r}
n <- length(x)
t <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
as.numeric(t)
```

*We can see that the t value above is exactly same as the t-statistic given in the summary of "r11_a".*

(e) Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y.

*From the formula in (d), if we replace $x_i$ with $y_i$ and replace $y_i$ with $x_i$ for t-statistics, the result would be the same.*

(f) In R, show that when regression is performed with an intercept, the t-statistic is the same for the regression of y onto x as it is for the regression of x onto y.

```{r}
# the regression of y onto x
r11_f1 <- lm(y ~ x)
summary(r11_f1)

# the regression of x onto y
r11_f2 <- lm(x ~ y)
summary(r11_f2)
```

*From the summary results, we can see that the t-statistic is the same for the regression of y onto x as it is for the regression of x onto y, which is equal to 18.56.*

## 3.12

This problem involves simple linear regression without an intercept.

(a) Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X?

*The coefficient estimate for the regression of Y onto X is*
$$\hat{\beta}=\frac{\sum_{i}x_iy_i}{\sum_{j}x_j^2}$$
*The coefficient estimate for the regression of X onto Y is*
$$\hat{\beta'}=\frac{\sum_{i}x_iy_i}{\sum_{j}y_j^2}$$

*We can see from the above two formulas the two coefficients are the same if $\sum_{j}x_j^2=\sum_{j}y_j^2$.*

(b) Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.

```{r}
set.seed(1)
x <- 1:100
sum(x^2)
y <- 2 * x + rnorm(100)
sum(y^2)
```

*We can see $\sum_{j}x_j^2$ is different from $\sum_{j}y_j^2$*

```{r}
r12_b1 <- lm(y ~ x + 0)
r12_b2 <- lm(x ~ y + 0)
summary(r12_b1)
summary(r12_b2)
```

*From summary results, we can see that the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.*

(c) Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X.

```{r}
set.seed(1)
x <- 1:100
sum(x^2)
y <- 100:1
sum(y^2)
```

*We can see $\sum_{j}x_j^2$ is the same as $\sum_{j}y_j^2$*

```{r}
r12_c1 <- lm(y ~ x + 0)
r12_c2 <- lm(x ~ y + 0)
summary(r12_c1)
summary(r12_c2)
```

*From summary results, we can see that the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X.*

## 3.13

(a) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0,1) distribution. This represents a feature, X.

```{r}
set.seed(1)
x <- rnorm(100)
```

(b) Using the rnorm() function, create a vector, “eps”, containing 100 observations drawn from a N(0,0.25) distribution.

```{r}
eps <- rnorm(100, sd=sqrt(0.25))
```

(c) 

```{r}
y <- -1 + 0.5 * x + eps
paste("the length of the vector y is", as.character(length(y)), sep = " ")
```

*In this linear model, $\beta_0=-1$, $\beta_1=0.5$.*

(d) Create a scatterplot displaying the relationship between x and y. Comment on what you observe.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(data = as.data.frame(y,x,eps))+
  geom_point(aes(x,y))
```

*There exists a linear relationship between x and y with some noise introduced by the eps variable.*

(e) 

```{r}
r13_e <- lm(y ~ x)
summary(r13_e)
```

*The p-values for t-statistics are pretty small, which indicates the coefficients in the linear model are significant.*

*$\hat{\beta_0}=-1.01885, \hat{\beta_1}=0.49947$*

*The values of $\hat{\beta_0}$ and $\hat{\beta_1}$ are pretty close to the values of ${\beta_0}$ and ${\beta_1}$.*

(f) 

```{r}
plot(x, y)
abline(r13_e, col = "red")
abline(-1, 0.5, col = "blue")
legend("topleft", c("Least Squares Line", "Population Regression Line"), col = c("red", "blue"), lty = c(1, 1))
```

(g) Now fit a polynomial regression model

```{r}
r13_g <- lm(y ~ poly(x,2,raw = TRUE))
# r13_g <- lm(y ~ x + I(x^2))
summary(r13_g)
```

*The coefficient of $x^2$ is not significant, so there is not sufficient evidence to show the quadratic term can improve the model even if the $R^2$ is little higher.*

(h) Repeat (a)-(f) after modifying the data generation process in such a way that there is less noise in the data. The initial model should remain the same. Describe your results.

```{r}
# generate variables
set.seed(1)
eps <- rnorm(100, sd = 0.1)
x <- rnorm(100)
y <- -1 + 0.5 * x + eps

# regression
r13_h <- lm(y ~ x)
summary(r13_h)

# plot the relationship
plot(x, y)
abline(r13_h, col = "red")
abline(-1, 0.5, col = "blue")
legend("topleft", c("Least Squares Line", "Population Regression Line"), col = c("red", "blue"), lty = c(1, 1))

```

*We reduced the noise by decreasing the variance of the normal distribution used to generate the error term $\epsilon$. We may see that the coefficients are very close to those in population regression. Besides, $R^2$ is much higher and RSE is much lower than the previous one. Moreover, the two lines are very close to each other.*

(i) Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data.

```{r}
# generate variables
set.seed(1)
eps <- rnorm(100, sd = 0.8)
x <- rnorm(100)
y <- -1 + 0.5 * x + eps

# regression
r13_i <- lm(y ~ x)
summary(r13_i)

# plot the relationship
plot(x, y)
abline(r13_i, col = "red")
abline(-1, 0.5, col = "blue")
legend("topleft", c("Least Squares Line", "Population Regression Line"), col = c("red", "blue"), lty = c(1, 1))
```

*We increased the noise by increasing the variance of the normal distribution used to generate the error term $\epsilon$. We can see that the coefficients are close to those in population regression. However, $R^2$ is much lower and RSE is much higher than the previous one. Moreover, the two lines are wider apart but are still close to each other.*

(j) confidence intervals 

```{r}
# the original data set
confint(r13_e)

# the less noisy data set
confint(r13_h)

# the noisier data set
confint(r13_i)
```

*Larger the deviation of noise is, wider the confidence intervals are. With less noise, there is more predictability in the data set.*

## 3.14

(a) The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?

```{r}
set.seed (1)
x1=runif (100)
x2 =0.5* x1+rnorm (100) /10
y=2+2* x1 +0.3* x2+rnorm (100)
```

*The form of the linear model is:*
$$Y=2+2X_1+0.3X_2+\epsilon$$

*$\beta_0=2$, $\beta_1=2$, $\beta_2=0.3$.*

(b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.

```{r}
cor(x1,x2)
plot(x1,x2)
```

*From both correlation value and scatterplot, we can see x1 and x2 is highly correlated with each other.*

(c) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained.

```{r}
r14_c <- lm(y ~ x1 + x2)
summary(r14_c)
```

*$\hat{\beta_0}=2.1305$, $\hat{\beta_1}=1.4396$, $\hat{\beta_2}=1.0097$. Only the value of $\hat{\beta_0}$ is very close to that of $\beta_0$. Because the p-value for $\beta_1$ is smaller than 0.05, we can reject the null hypothesis. However, the p-value for $\beta_2$ is bigger than 0.05, so we cannot reject $H_0$.*

(d) Now fit a least squares regression to predict y using only x1. Comment on your results.

```{r}
r14_d <- lm(y ~ x1)
summary(r14_d)
```

*The coefficient for x1 in this model is very different from that in (c). In this case because the p-value for x1 is very low, we can reject $H_0$, which indicates that x1 is highly significant.*

(e) Now fit a least squares regression to predict y using only x2. Comment on your results.

```{r}
r14_e <- lm(y ~ x2)
summary(r14_e)
```

*The coefficient for x2 in this model is very different from that in (c). In this case because the p-value for x2 is very low, we can reject $H_0$, which indicates that x2 is highly significant.*

(f) Do the results obtained in (c)–(e) contradict each other? Explain your answer.

*No, the results do not contradict each other. As the predictors x1 and x2 are highly correlated, there exists collinearity between x1 and x2. In this case it can be difficult to determine how each predictor separately is associated with the response. Since collinearity reduces the accuracy of the coefficients estimation. Consequently, we may fail to reject $H_0$ in the presence of collinearity. The importance of the x2 variable has been masked due to the presence of collinearity.*


(g)

```{r}
x1=c(x1 , 0.1)
x2=c(x2 , 0.8)
y=c(y,6)
```

```{r}
r14_c2 <- lm(y ~ x1 + x2)
r14_d2 <- lm(y ~ x1)
r14_e2 <- lm(y ~ x2)
summary(r14_c2)
summary(r14_d2)
summary(r14_e2)
```

```{r}
plot(r14_c2)
plot(r14_d2)
plot(r14_e2)
```


*In the model with two predictors, the last point is a high-leverage point. In the model with x1 as sole predictor, the last point is an outlier. In the model with x2 as sole predictor, the last point is a high-leverage point.*



